{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "474321d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8e093f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\words.zip.\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data] Error loading averaged_preception_tagger: Package\n",
      "[nltk_data]     'averaged_preception_tagger' not found in index\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('words')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_preception_tagger')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "56ed088a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Initialize the text \n",
    "text= \"Tokenization is the first step in text analytics. The process of breaking down a text paragraph into smaller chunks such as words or sentences is called Tokenization.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ac9cf70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Tokenization is the first step in text analytics.', 'The process of breaking down a text paragraph into smaller chunks such as words or sentences is called Tokenization.']\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Perform Tokenization \n",
    "#Sentence Tokenization \n",
    "from nltk.tokenize import sent_tokenize \n",
    "tokenized_text= sent_tokenize(text) \n",
    "print(tokenized_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb8b514",
   "metadata": {},
   "source": [
    "splitting up a larger body of text into smaller lines, words or even creating words for a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "057fe5fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Tokenization', 'is', 'the', 'first', 'step', 'in', 'text', 'analytics', '.', 'The', 'process', 'of', 'breaking', 'down', 'a', 'text', 'paragraph', 'into', 'smaller', 'chunks', 'such', 'as', 'words', 'or', 'sentences', 'is', 'called', 'Tokenization', '.']\n"
     ]
    }
   ],
   "source": [
    "#Word Tokenization \n",
    "from nltk.tokenize import word_tokenize \n",
    "tokenized_word=word_tokenize(text) \n",
    "print(tokenized_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c28eadf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'how', 'further', 'from', \"wasn't\", 'him', 'again', 'himself', 'ain', 'shan', 'them', 'over', 'mightn', 'if', 'now', 'most', 'her', 'mustn', \"it's\", 'hers', 'about', \"don't\", 'she', 'doesn', 'has', \"weren't\", 'while', 'we', 'against', 'or', 'and', 'a', 'that', 'too', 'is', 'few', \"you'll\", 'before', 'his', 'our', 'wouldn', 'be', 'their', 'between', 'than', 'why', 'm', 'needn', 'after', 'once', 'my', 'any', \"wouldn't\", \"needn't\", 'ma', 'shouldn', \"haven't\", 'isn', 'here', 'those', 'herself', \"you're\", 'into', \"aren't\", 'these', 'were', 'itself', 'am', \"mightn't\", 'wasn', \"that'll\", 'so', 'won', \"won't\", 'ours', 'an', 'both', \"didn't\", 'just', 'hasn', 'yourself', 'by', 'because', 'what', 'which', 'was', 'as', 've', 'in', 'such', 'y', 'your', 'themselves', 'doing', 'up', 'of', 'some', 'does', 'to', 'have', 'then', 'when', 'they', 'having', 'me', 'very', 'weren', 'can', 'don', \"couldn't\", 'but', 'did', 'off', 'being', 'yourselves', \"shouldn't\", 'its', 'will', 'there', 'you', \"isn't\", \"you'd\", 'during', 'at', 'do', 'haven', \"hadn't\", \"she's\", 'other', 'nor', 'below', 'for', 'whom', 'more', 'hadn', 'each', 'until', 'out', 's', 'didn', 're', \"you've\", 'aren', 'no', 'above', 'he', 'o', 'ourselves', 'same', 'couldn', 'it', 'are', 'where', \"should've\", 'through', 'this', 'down', 'all', 'with', 'theirs', 'yours', 'only', 'been', 'on', 'not', 't', \"mustn't\", 'd', \"doesn't\", \"shan't\", 'i', 'own', 'll', \"hasn't\", 'who', 'should', 'the', 'myself', 'had', 'under'}\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Removing Punctuations and Stop Word \n",
    "# print stop words of English \n",
    "from nltk.corpus import stopwords \n",
    "stop_words=set(stopwords.words(\"english\")) \n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4affc3a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized Sentence: ['how', 'to', 'remove', 'stop', 'words', 'with', 'nltk', 'library', 'in', 'python']\n",
      "Filterd Sentence: ['remove', 'stop', 'words', 'nltk', 'library', 'python']\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Removing Punctuations and Stop Word \n",
    "import re\n",
    "text = \"How to remove stop words with NLTK library in Python?\" \n",
    "text = re.sub('[^a-zA-Z]', ' ',text) \n",
    "tokens = word_tokenize(text.lower()) \n",
    "filtered_text=[] \n",
    "for w in tokens: \n",
    "  if w not in stop_words: \n",
    "    filtered_text.append(w) \n",
    "print(\"Tokenized Sentence:\",tokens) \n",
    "print(\"Filterd Sentence:\",filtered_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "df267dda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wait\n"
     ]
    }
   ],
   "source": [
    "# Step 5 : Perform Stemming \n",
    "from nltk.stem import PorterStemmer \n",
    "e_words= [\"wait\", \"waiting\", \"waited\", \"waits\"] \n",
    "ps =PorterStemmer() \n",
    "for w in e_words: \n",
    "  rootWord=ps.stem(w) \n",
    "print(rootWord)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "848e973c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Stemming is a technique used to extract the base form of the words by removing affixes from them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "754ca6ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemma for studies is study\n",
      "Lemma for studying is studying\n",
      "Lemma for cries is cry\n",
      "Lemma for cry is cry\n"
     ]
    }
   ],
   "source": [
    "# Step 6: Perform Lemmatization \n",
    "from nltk.stem import WordNetLemmatizer \n",
    "wordnet_lemmatizer = WordNetLemmatizer() \n",
    "text = \"studies studying cries cry\" \n",
    "tokenization = nltk.word_tokenize(text) \n",
    "for w in tokenization: \n",
    "    print(\"Lemma for {} is {}\".format(w, wordnet_lemmatizer.lemmatize(w))) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d026b38f",
   "metadata": {},
   "source": [
    "Lemmatization is a text pre-processing technique used in natural language processing (NLP) models to break a word down to its root meaning to identify similarities. \n",
    "Lemmatization has higher accuracy than stemming. Lemmatization is preferred for context analysis, whereas stemming is recommended when the context is not important.\n",
    " lemmatization would correctly identify the base form of ‘caring’ to ‘care’, whereas, stemming would cutoff the ‘ing’ part and convert it to car. ‘Caring’ -> Lemmatization -> ‘Care’ ‘Caring’ -> Stemming -> ‘Car’"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "843bfb82",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'DT')]\n",
      "[('pink', 'NN')]\n",
      "[('sweater', 'NN')]\n",
      "[('fit', 'NN')]\n",
      "[('her', 'PRP$')]\n",
      "[('perfectly', 'RB')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n"
     ]
    }
   ],
   "source": [
    "# Step 7: Apply POS Tagging to text \n",
    "import nltk \n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from nltk.tokenize import word_tokenize \n",
    "data=\"The pink sweater fit her perfectly\" \n",
    "words=word_tokenize(data) \n",
    "for word in words: \n",
    "    print(nltk.pos_tag([word]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "814dfe4b",
   "metadata": {},
   "source": [
    "part-of-speech (POS) tagging on individual words\n",
    "DT: article\n",
    "NN: noun\n",
    "PRP$: pronoun\n",
    "RB: adverb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f1fee438",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Algorithm for Create representation of document by calculating TFIDF "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f149e11c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Import the necessary libraries. \n",
    "import pandas as pd \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0148f251",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Initialize the Documents. \n",
    "documentA = 'Jupiter is the largest Planet' \n",
    "documentB = 'Mars is the fourth planet from the Sun' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3d05571b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Create BagofWords (BoW) for Document A and B. \n",
    "bagOfWordsA = documentA.split(' ') \n",
    "bagOfWordsB = documentB.split(' ') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "46c36179",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Create Collection of Unique words from Document A and B. \n",
    "uniqueWords = set(bagOfWordsA).union(set(bagOfWordsB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "761962cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Create a dictionary of words and their occurrence for each document in the corpus \n",
    "numOfWordsA = dict.fromkeys(uniqueWords, 0) \n",
    "for word in bagOfWordsA: \n",
    "    numOfWordsA[word] += 1 \n",
    "numOfWordsB = dict.fromkeys(uniqueWords, 0) \n",
    "for word in bagOfWordsB: \n",
    "    numOfWordsB[word] += 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4175d6ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Compute the term frequency for each of our documents. \n",
    "def computeTF(wordDict, bagOfWords): \n",
    "    tfDict = {} \n",
    "    bagOfWordsCount = len(bagOfWords) \n",
    "    for word, count in wordDict.items(): \n",
    "        tfDict[word] = count / float(bagOfWordsCount) \n",
    "    return tfDict \n",
    "tfA = computeTF(numOfWordsA, bagOfWordsA) \n",
    "tfB = computeTF(numOfWordsB, bagOfWordsB) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94784727",
   "metadata": {},
   "source": [
    "Term Frequency: The number of times a word appears in a document divded by the total number of words in the document. Every document has its own term frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2633e771",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fourth': 0.6931471805599453,\n",
       " 'largest': 0.6931471805599453,\n",
       " 'Planet': 0.6931471805599453,\n",
       " 'from': 0.6931471805599453,\n",
       " 'the': 0.0,\n",
       " 'Sun': 0.6931471805599453,\n",
       " 'is': 0.0,\n",
       " 'planet': 0.6931471805599453,\n",
       " 'Jupiter': 0.6931471805599453,\n",
       " 'Mars': 0.6931471805599453}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 7: Compute the term Inverse Document Frequency. \n",
    "def computeIDF(documents): \n",
    "    import math \n",
    "    N = len(documents) \n",
    "    idfDict = dict.fromkeys(documents[0].keys(), 0) \n",
    "    for document in documents: \n",
    "        for word, val in document.items(): \n",
    "            if val > 0: \n",
    "                idfDict[word] += 1 \n",
    "    for word, val in idfDict.items(): \n",
    "        idfDict[word] = math.log(N / float(val)) \n",
    "    return idfDict \n",
    "idfs = computeIDF([numOfWordsA, numOfWordsB]) \n",
    "idfs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ee98ef",
   "metadata": {},
   "source": [
    "The log of the number of documents divided by the number of documents that contain the word w. Inverse data frequency determines the weight of rare words across all documents in the corpus.\n",
    "https://towardsdatascience.com/natural-language-processing-feature-engineering-using-tf-idf-e8b9d00e7e76"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b05af925",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fourth</th>\n",
       "      <th>largest</th>\n",
       "      <th>Planet</th>\n",
       "      <th>from</th>\n",
       "      <th>the</th>\n",
       "      <th>Sun</th>\n",
       "      <th>is</th>\n",
       "      <th>planet</th>\n",
       "      <th>Jupiter</th>\n",
       "      <th>Mars</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.138629</td>\n",
       "      <td>0.138629</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.138629</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.086643</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.086643</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.086643</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.086643</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.086643</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     fourth   largest    Planet      from  the       Sun   is    planet   \n",
       "0  0.000000  0.138629  0.138629  0.000000  0.0  0.000000  0.0  0.000000  \\\n",
       "1  0.086643  0.000000  0.000000  0.086643  0.0  0.086643  0.0  0.086643   \n",
       "\n",
       "    Jupiter      Mars  \n",
       "0  0.138629  0.000000  \n",
       "1  0.000000  0.086643  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 8: Compute the term TF/IDF for all words. \n",
    "def computeTFIDF(tfBagOfWords, idfs): \n",
    "    tfidf = {} \n",
    "    for word, val in tfBagOfWords.items(): \n",
    "        tfidf[word] = val * idfs[word] \n",
    "    return tfidf \n",
    "tfidfA = computeTFIDF(tfA, idfs) \n",
    "tfidfB = computeTFIDF(tfB, idfs) \n",
    "df = pd.DataFrame([tfidfA, tfidfB]) \n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a86e9c9",
   "metadata": {},
   "source": [
    "a document typically refers to a unit of text that is being processed or analyzed. A document can represent a piece of text such as a single sentence, a paragraph, an article, or an entire book \n",
    " Inverse data frequency determines the weight of rare words across all documents in the corpus."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
